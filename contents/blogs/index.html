<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>LLMs 101: Training an LLM from Scratch | By Waleed Esmail</title>
        <meta name="description" content="LLMs 101: Training an LLM from Scratch">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        
        <!-- Favicon -->
        <link rel="icon" type="image/svg+xml" href="../../assets/images/astronomy-education-science-svgrepo-com.svg">
        <link rel="icon" type="image/x-icon" href="../../assets/images/favicon.ico">
        <link rel="icon" type="image/png" sizes="32x32" href="../../assets/images/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="../../assets/images/favicon-16x16.png">
        <link rel="apple-touch-icon" sizes="180x180" href="../../assets/images/apple-touch-icon.png">
        
        <link rel="stylesheet" href="style.css">
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.8/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-sRIl4kxILFvY47J16cr9ZwB07vP4J8+LH7qKQnuqkuIAvNWLzeN8tE5YBujZqJLB" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.8/dist/js/bootstrap.bundle.min.js" integrity="sha384-FKyoEForCGlyvwx9Hj09JcYn3nv7wiPVlz7YYwJrWVcXK/BmnVDxM+D2scQbITxI" crossorigin="anonymous"></script>
        <!-- Google Fonts -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&display=swap" rel="stylesheet">
        <!-- for syntax highlighting -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css" media="(prefers-color-scheme: dark)">
        
    </head>

        <!-- Make the header sticky and full-width background -->
        <header class="sticky-top bg-body-tertiary border-bottom">
        <div class="container">
            <div class="d-flex align-items-center py-3">

            <a href="/" class="d-flex align-items-center mb-0 me-auto link-body-emphasis text-decoration-none">
                <svg class="bi me-2" width="40" height="32" aria-hidden="true"><use xlink:href="#bootstrap"></use></svg>
                <span class="fs-3 fstyle-primary">Waleed Esmail</span>
            </a>

            <ul class="nav nav-pills nav-fill gap-2 p-1 small bg-primary-subtle rounded-5 shadow-sm" id="pillNav2" role="tablist" style="--bs-nav-link-color: var(--bs-white); --bs-nav-pills-link-active-color: var(--bs-primary); --bs-nav-pills-link-active-bg: var(--bs-white);">
            <li class="nav-item fw-semibold" role="presentation">
                <button class="nav-link rounded-5 fstyle-secondary" id="home-tab2" data-bs-toggle="tab" type="button" role="tab" aria-selected="false">
                    <a href="../../index.html#top">Home</a>
                </button>
            </li>
            <li class="nav-item fw-semibold" role="presentation">
                <button class="nav-link active rounded-5 fstyle-secondary" id="profile-tab2" data-bs-toggle="tab" type="button" role="tab" aria-selected="true">Blogs</button>
            </li>
            <li class="nav-item fw-semibold" role="presentation">
                <button class="nav-link rounded-5 fstyle-secondary" id="experience-tab2" data-bs-toggle="tab" type="button" role="tab" aria-selected="false"><a href="../../index.html#work-experience">Experience</a></button>
            </li>
            </ul>

            <!-- Theme toggle -->
            <button id="themeToggle" class="btn btn-outline-secondary btn-sm ms-3" type="button" aria-label="Toggle theme">ğŸŒ™</button>
            </div>
        </div>
        </header>


    <main class="container py-3">
    <div class="p-4 p-md-5 mb-4 rounded text-body-emphasis bg-body-secondary">
        <div class="col-lg-6 px-0">
            <h1 class="display-4 fst-italic">LLMs 101: Training an LLM from Scratch</h1>
            <p class="lead my-3"><strong>Large Language Models (LLMs)</strong> are everywhere nowadays. 
                They can perform a wide range of tasks, such as answering questions on various topics, 
                summarizing long articles, and, more interestingly,</p>
            <p class="lead mb-0"><a href="#content" class="text-body-emphasis fw-bold">Continue reading...</a></p>
        </div>
    </div>
    <!--
    <div class="row mb-2">
        <div class="col-md-6">
            <div class="row g-0 border rounded overflow-hidden flex-md-row mb-4 shadow-sm h-md-250 position-relative">
                <div class="col p-4 d-flex flex-column position-static">
                <strong class="d-inline-block mb-2 text-primary-emphasis">World</strong> 
                <h3 class="mb-0">Featured post</h3>
                <div class="mb-1 text-body-secondary">Nov 12</div>
                <p class="card-text mb-auto">This is a wider card with supporting text below as a natural lead-in to additional content.</p>
                <a href="#" class="icon-link gap-1 icon-link-hover stretched-link">
                    Continue reading
                    <svg class="bi" aria-hidden="true">
                        <use xlink:href="#chevron-right"></use>
                    </svg>
                </a>
                </div>
                <div class="col-auto d-none d-lg-block">
                <svg aria-label="Placeholder: Thumbnail" class="bd-placeholder-img " height="250" preserveAspectRatio="xMidYMid slice" role="img" width="200" xmlns="http://www.w3.org/2000/svg">
                    <title>Placeholder</title>
                    <rect width="100%" height="100%" fill="#55595c"></rect>
                    <text x="50%" y="50%" fill="#eceeef" dy=".3em">Thumbnail</text>
                </svg>
                </div>
            </div>
        </div>
        <div class="col-md-6">
            <div class="row g-0 border rounded overflow-hidden flex-md-row mb-4 shadow-sm h-md-250 position-relative">
                <div class="col p-4 d-flex flex-column position-static">
                <strong class="d-inline-block mb-2 text-success-emphasis">Design</strong> 
                <h3 class="mb-0">Post title</h3>
                <div class="mb-1 text-body-secondary">Nov 11</div>
                <p class="mb-auto">This is a wider card with supporting text below as a natural lead-in to additional content.</p>
                <a href="#" class="icon-link gap-1 icon-link-hover stretched-link">
                    Continue reading
                    <svg class="bi" aria-hidden="true">
                        <use xlink:href="#chevron-right"></use>
                    </svg>
                </a>
                </div>
                <div class="col-auto d-none d-lg-block">
                <svg aria-label="Placeholder: Thumbnail" class="bd-placeholder-img " height="250" preserveAspectRatio="xMidYMid slice" role="img" width="200" xmlns="http://www.w3.org/2000/svg">
                    <title>Placeholder</title>
                    <rect width="100%" height="100%" fill="#55595c"></rect>
                    <text x="50%" y="50%" fill="#eceeef" dy=".3em">Thumbnail</text>
                </svg>
                </div>
            </div>
        </div>
    </div>
    -->
    <div class="row g-5" id="content">
        <div class="col-md-8">
            <h3 class="pb-4 mb-4 fst-italic border-bottom">
                Latest Posts
            </h3>
            <article class="blog-post">
                    <h2 class="display-5 link-body-emphasis mb-1">LLMs 101: Training an LLM from Scratch</h2>
                    <p class="blog-post-meta">February 16, 2025</p>
                    <img src="../../assets/images/poster-blog-landing-page.webp" alt="LLMs 101: Training an LLM from Scratch" class="img-fluid rounded mb-4" style="width: 100%;">
                    <p>
                        <strong>Large Language Models (LLMs)</strong> are everywhere nowadays. 
                        They can perform a wide range of tasks, such as answering questions on various topics, 
                        summarizing long articles, and, more interestingly, reasoning and generating project ideas. 
                        In this article, I will guide you through training a simple LLM, specifically, the <strong>GPT-2</strong> model.
                    </p>
                      
                    <p>
                        Today, most LLMs on the market are based on the transformer architecture, 
                        first introduced in the seminal paper <em>"Attention Is All You Need."</em> 
                        Although I won't dive into the detailed workings of transformers, 
                        since there are many excellent tutorials covering the technical aspects, 
                        I will show you how to select a transformer-based model off the shelf and train it on your own dataset.
                    </p>
                      
                    <p>
                        For this tutorial, I will use <strong>Hugging Face</strong> ğŸ¤—, 
                        a central hub for the machine learning and <strong>natural language processing (NLP)</strong> community. 
                        Hugging Face ğŸ¤— provides essential tools, models, datasets, and libraries that simplify and accelerate 
                        the development of AI applications, particularly those involving transformers and LLMs.
                    </p>
                      
                    <p>
                        The goal of this article is to keep things simple and concise. 
                        So, let's jump right in!
                    </p>
                    <hr>
                    <h2 class="fstyle-secondary">ğŸ› ï¸ Step 0: Installing the Required Packages</h2> 
                    <p>Before we dive into training our LLM, let's set up our environment by installing the necessary packages. 
                       You have a couple of options for installation, depending on your preferred workflow.
                    </p>
                    <strong>ğŸ’¡ Which Package Manager Should You Use?</strong>
                    <p>
                        You can choose between two popular tools for managing your Python environment and packages:
                    </p>
                      
                    <ul>
                        <li>ğŸª¶ <strong>micromamba</strong>: A lightweight, fast, and minimal package and environment manager for Python. Itâ€™s an excellent alternative to Conda and ideal for managing isolated environments.</li>
                        <li>ğŸš€ <strong>pip</strong>: The official package manager for Python, suitable for quick installations and virtual environments.</li>
                    </ul>
                      
                    <strong>ğŸ“¦ Required Packages:</strong>
                      
                    <p>We'll need the following libraries:</p>
                      
                    <ul>
                        <li>âœ… <strong>NumPy</strong> â€” <a href="https://numpy.org/install/" target="_blank">https://numpy.org/install/</a></li>
                        <li>âœ… <strong>pandas</strong> â€” <a href="https://pandas.pydata.org/docs/getting_started/install.html" target="_blank">https://pandas.pydata.org/docs/getting_started/install.html</a></li>
                        <li>âœ… <strong>matplotlib</strong> â€” <a href="https://matplotlib.org/stable/install/index.html" target="_blank">https://matplotlib.org/stable/install/index.html</a></li>
                        <li>âœ… <strong>ğŸ¤— Transformers</strong> â€” <a href="https://huggingface.co/docs/transformers/en/installation" target="_blank">https://huggingface.co/docs/transformers/en/installation</a></li>
                    </ul>
                      
                    <h2 class="fstyle-secondary">ğŸ“Š Step 1: Obtaining the Dataset</h2>
                    <p>The choice of the dataset is entirely up to you! With over 300,000 datasets available on ğŸ¤— <a href="https://huggingface.co/datasets" target="_blank">Datasets</a>, 
                        you can find one that suits your interests and goals. Whether you're interested in literature, science, 
                        or social trends, ğŸ¤— Datasets has you covered!
                    </p>
                    <p>Since my background is in physics, I'll use a physics-related dataset available on ğŸ¤— Datasets. 
                        Specifically, I've chosen:
                    </p>
                    <p>
                    ğŸ“š <a href="https://huggingface.co/datasets/enesxgrahovac/the-feynman-lectures-on-physics">The Feynman Lectures on Physics</a> 
                    by <a href="https://huggingface.co/enesxgrahovac" target="_blank">Enes Grahovac</a>
                    </p>
                    <p>This dataset contains excerpts from one of the most iconic physics textbook ever written.
                      To get the dataset, go to the Feynman Lectures on Physics dataset page. 
                      On the dataset page, click the <strong>use this dataset</strong> button and choose the <strong>pandas</strong> option. 
                      Then copy the link and use pandas to load the dataset:
                    </p>
                     <div class="code-block">
                         <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                                 onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                             <span class="d-none d-dark">â˜€ï¸</span>
                             <span class="d-none d-light">ğŸŒ™</span>
                             Copy
                         </button>
                         <pre class="bg-body-tertiary rounded"><code class="code-block language-python">import pandas as pd
df = pd.read_parquet("hf://datasets/enesxgrahovac/the-feynman-lectures-on-physics/data/train-00000-of-00001-d61a336a5c880eb1.parquet")
</code></pre>
                     </div>

                    <p>
                        The dataset contains <strong>7 columns</strong>, capturing excerpts from the renowned physics textbook 
                        <em>Feynman Lectures on Physics</em> (though it doesn't include the entire book). Among these columns, the most valuable for our LLM training is:
                    </p>
                        <ul>
                            <li><strong><code>section_text</code></strong>: This column contains segments of text from the lectures, which we will use as training data for our language model.</li>
                        </ul>
                        
                    <p>
                        To count the total number of words in the <code>section_text</code> column, we can use the following code:                        
                    </p>
                        <div class="code-block language-python">
                            <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                                    onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                                <span class="d-none d-dark">â˜€ï¸</span>
                                <span class="d-none d-light">ğŸŒ™</span>
                                Copy
                            </button>
                            <pre class="bg-body-tertiary rounded"><code class="language-python"># Count total words in the 'section_text' column
total_words = df["section_text"].apply(lambda x: len(str(x).split())).sum()
# Print the result
print(f"Text size: {total_words} words")
</code></pre>
                        </div>

                        <h2 class="fstyle-secondary">ğŸ“– Step 2: Tokenization</h2>
                        <p>This dataset contains more than 724,000 words, which is a good amount of text to train our <em>little</em> LLM! 
                        ğŸš€ Now, it's time for tokenization, a crucial step in transforming raw text into a format that the model can understand.
                        </p>
                        <p>
                            Tokenization is simply the process of converting text into numbers, breaking down sentences into smaller units (<strong>tokens</strong>) that the model can process.
                        </p>   
                        
                        <strong>ğŸ¤– Choosing the Right Tokenizer</strong>
                        <p>
                            Since we're using GPT-2, we'll use the <code>GPT2Tokenizer</code> from ğŸ¤— 
                            <a href="https://huggingface.co/docs/transformers/en/installation">transformers library</a>. 
                            ğŸ¤— transformers provides a convenient interface to load either:
                        </p>
                        <ul>
                            <li>âœ… A pre-trained tokenizer â€” Already trained on massive datasets and optimized for GPT-2.</li>
                            <li>âœ… An untrained tokenizer â€” Can be trained from scratch if you want a completely custom tokenization strategy.</li>                            
                        </ul>
                        <p>For simplicity, we will start with the pre-trained tokenizer.</p>
                    <div class="code-block language-python">
                        <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                                onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                            <span class="d-none d-dark">â˜€ï¸</span>
                            <span class="d-none d-light">ğŸŒ™</span>
                            Copy
                        </button>
                        <pre class="bg-body-tertiary rounded"><code class="language-python">from transformers import GPT2Tokenizer

# Load the GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
# Print tokenizer details
print(tokenizer)
</code></pre>
                    </div>

                    <p>
                        The tokenizer outputs key details about its configuration:
                      </p>
                      
                      <ul>
                        <li>ğŸ§© <strong>Vocabulary Size:</strong> 50,257 tokens (unique token IDs)</li>
                        <li>ğŸ§  <strong>Maximum Context Length:</strong> 1,024 tokens. This is the maximum input size for GPT-2.</li>
                        <li>ğŸ·ï¸ <strong>Special Tokens:</strong></li>
                        <ul>
                          <li><code>&lt;|endoftext|&gt;</code> â€“ Used as:</li>
                          <li><code>bos_token</code> (Beginning of Sequence)</li>
                          <li><code>eos_token</code> (End of Sequence)</li>
                          <li><code>unk_token</code> (Unknown Token)</li>
                          <li>ğŸ“Œ All special tokens are set to the same identifier: <code>'&lt;|endoftext|&gt;'</code></li>
                        </ul>
                      </ul>
                      
                      <p>
                        Let's see how the tokenizer processes our dataset by tokenizing the first row of <code>section_text</code>:
                      </p>
                    <div class="code-block language-python">
                        <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                                onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                            <span class="d-none d-dark">â˜€ï¸</span>
                            <span class="d-none d-light">ğŸŒ™</span>
                            Copy
                        </button>
                        <pre class="bg-body-tertiary rounded"><code class="language-python"># Tokenize the first section without truncation
tokenized_output = tokenizer(df["section_text"].iloc[0])
print(tokenized_output)
</code></pre>
                    </div>
                    <p>
                        The tokenizer returns a <strong>dictionary</strong> containing:
                      </p>
                      
                      <ul>
                        <li>ğŸ“š <code>input_ids</code>: A list of token IDs representing the text.</li>
                        <li>ğŸ’¡ <code>attention_mask</code>: Can you guess what this is?</li>
                      </ul>
                      
                      <p>
                        Did you notice something? ğŸ«¢
                      </p>
                      
                      <p>
                        The <strong>text length exceeds the tokenizerâ€™s context window of 1,024 tokens</strong>, which means the model cannot process the entire text at once. This could cause:
                      </p>
                      
                      <ul>
                        <li>âŒ <strong>Indexing errors (out-of-bounds errors):</strong> When the model tries to access tokens beyond the maximum input length.</li>
                      </ul>
                      
                      <p>
                        To address this, we can instruct the tokenizer to automatically truncate any input that exceeds the context size using the <code>truncation=True</code> argument:
                      </p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
                <pre class="bg-body-tertiary rounded"><code class="language-python"># Tokenize with truncation enabled
tokenized_truncated = tokenizer(df["section_text"].iloc[0], truncation=True)

# Print the truncated tokenized output
print(tokenized_truncated)
</code></pre>
            </div>
            <p>
                ğŸ“š The histogram below displays the distribution of section lengths based on word count, and the red ğŸ’… vertical line marks the <strong>GPT-2 context limit (1,024 tokens)</strong>. Youâ€™ll likely notice that <strong>a significant portion of the text exceeds the context window</strong>, meaning some sections are too long for the model to handle in a single pass.
              </p>
              
              <p style="text-align:center;">
                <img src="./assets/images/histogram_word_counts.webp" alt="Distribution of Word Counts in Section Text" style="max-width:90%; border:1px solid #ccc; border-radius:6px;">
              </p>
              
              <p style="text-align:center; font-weight:bold;">
                Distribution of Word Counts in Section Text
              </p>
              <p>
                Since many sections exceed the modelâ€™s input limit, we will split longer texts into smaller chunks before training. 
                For example, we can split the text by periods (<code>"."</code>) or newline characters (<code>"\n"</code>) 
                to create manageable chunks. 
                This is how it is done:
              </p>
              
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
                <pre class="bg-body-tertiary rounded"><code class="language-python"># Splitting the long section by sentences based on "."
df["section_text_split"] = df["section_text"].str.split(".")
df_exploded = df.explode("section_text_split")
</code></pre>
            </div>
            <h2 class="fstyle-secondary">ğŸš€ Step 3: Initializing the GPT-2 Model for Training</h2>
            <p>
                We are now ready to train our LLM! ğŸ‰ 
                As mentioned earlier, we will use the <strong>GPT-2 model</strong>, 
                but I won't dive into the model's internals here, as there are many excellent resources that explain its architecture in detail. 
                If you'd like to understand the theory and inner workings of <strong>GPT-2</strong>, I highly recommend this excellent tutorial:
              </p>
              
              <p>
                ğŸ‘‰ ğŸ“– 
                <a href="https://medium.com/@vipul.koti333/from-theory-to-code-step-by-step-implementation-and-code-breakdown-of-gpt-2-model-7bde8d5cecda" target="_blank">
                  From Theory to Code: Step-by-Step Implementation and Code Breakdown of GPT-2 model</a> 
                by Vipul Koti
              </p>
              
              <p>
                ğŸ§  <strong>Want to Train a Different LLM?</strong><br>
                ğŸ¤— offers a vast collection of LLM models. Feel free to explore and experiment! ğŸš€
              </p>
              
              <p>
                ğŸ‘‰ ğŸ”— <strong>Browse available models:</strong> 
                ğŸ¤— <a href="https://huggingface.co/models" target="_blank">Models</a>
              </p>
              
              <p>
                We will use the <code>GPT2LMHeadModel</code>, which is the 
                <strong>GPT-2 model with a language modeling head (LM head)</strong>. 
                The LM head is a linear layer that outputs a tensor of the same shape as the tokenized input, 
                allowing the model to predict the next token in a sequence.
              </p>
              <p style="text-align:center;">
                <img src="./assets/images/gpt2_model_diagram.jpg" alt="GPT-2 Model Architecture" style="max-width:40%; border:1px solid #ccc; border-radius:6px; margin-bottom:8px;">
              </p>
              
              <p style="text-align:center; font-size:0.9em; color:#555;">
                GPT-2 Model with the LM head. <em>Credit:</em> 
                <a href="https://medium.com/@vipul.koti333/from-theory-to-code-step-by-step-implementation-and-code-breakdown-of-gpt-2-model-7bde8d5cecda" target="_blank">
                    Vipul Koti</a>.
              </p>
              
              <p>
                Let's start by importing the necessary modules from ğŸ¤— transformers:
              </p>

            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
                <pre class="bg-body-tertiary rounded"><code class="language-python"># Importing Hugging Face Transformers modules
from transformers import GPT2Config, GPT2LMHeadModel, TrainingArguments, Trainer
</code></pre>
            </div>
             <p>Next, we will create a <code>GPT2LMHeadModel</code> from scratch using the <code>GPT2Config</code> class and move it to the GPU if available:</p>
             <p>
                âœ… Here, <code>GPT2Config()</code> Initializes the model with default GPT-2 configurations 
                <ul>
                  <li><strong>Layers:</strong> 12</li>
                  <li><strong>Hidden dimensions:</strong> 768</li>
                  <li><strong>Attention heads:</strong> 12</li>
                </ul>
                You can simply use <code>print(GPT2Config())</code> to investigate the contents.
              </p>
              
              <p>
                âš ï¸ <strong>Note on Using a Pre-Trained Model:</strong>
              </p>
              
              <p>
                If you want to fine-tune a <strong>pre-trained GPT-2</strong> instead of training from scratch, simply use:
              </p>
                           
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python">from transformers import GPT2LMHeadModel

# Load a pre-trained GPT-2 model from Hugging Face
model = GPT2LMHeadModel.from_pretrained("gpt2")
# Move model to GPU if available
model = model.to(device)
</code></pre>
            </div>
            <h2 class="fstyle-secondary">ğŸš€ Step 4: Training Setup â€” Defining Arguments, Tokenizing the Dataset, and Preparing for Training</h2>
            <p>First, we define the hyperparameters for the training process using <code>TrainingArguments</code> ğŸ¤— transformers.</p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
                <pre class="bg-body-tertiary rounded"><code class="language-python">from transformers import TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2_pretrained",  # Directory to save model checkpoints
    overwrite_output_dir=True,       # Overwrite old checkpoints
    per_device_train_batch_size=8,   # Number of samples per batch per GPU
    num_train_epochs=5,              # Number of times the model will see the full dataset
    logging_dir="./logs",            # Directory for logs
    save_strategy="epoch",           # Save model after each epoch
    report_to="none",                # Disable logging to Weights & Biases
    fp16=True,                       # Enable mixed-precision training (faster on modern GPUs)
)
</code></pre>
            </div>
            <p><strong>Explanation of Training Arguments:</strong></p>

            <ul>
              <li>ğŸ“‚ <code>output_dir="./gpt2_pretrained"</code> : The directory where trained model checkpoints will be saved.</li>
              <li>ğŸ§¹ <code>overwrite_output_dir=True</code> :  If enabled, overwrites previous checkpoints (useful for experiments).</li>
              <li>ğŸš€ <code>per_device_train_batch_size=8</code> : Number of training samples processed per GPU. (Adjust based on your VRAM)</li>
              <li>ğŸ” <code>num_train_epochs=5</code> : Number of passes through the full dataset.</li>
              <li>ğŸ“š <code>logging_dir="./logs"</code> : Saves logs for tracking training progress.</li>
              <li>ğŸ’¾ <code>save_strategy="epoch"</code> : Saves model checkpoints at the end of every epoch.</li>
              <li>ğŸ§© <code>report_to="none"</code> : Prevents logging to third-party services like Weights & Biases.</li>
              <li>âš¡ <code>fp16=True</code> : Enables <strong>mixed-precision training</strong>, which speeds up training and reduces memory usage on GPUs that support it (e.g., NVIDIA 4090).</li>
            </ul>
            
            <p>
              Now, let's convert our <code>DataFrame</code> into a ğŸ¤— Dataset, tokenize it, and prepare it for training.
            </p>
            
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python">from transformers import AutoTokenizer
from datasets import Dataset

# Convert DataFrame to Hugging Face Dataset. I use only 1000 rows for simplicity
dataset = Dataset.from_pandas(df_exploded[['section_text_split']].iloc[0:1000])
# Rename the column to 'text' to match model expectations
dataset = dataset.rename_column("section_text_split", "text")
</code></pre>
            </div>
            <p>By default, GPT-2 does not use a <code>pad_token</code>, as it was originally trained without one. 
                However, during training and batching, padding is essential to ensure that all input sequences are of the same length. 
                Without proper padding, we cannot efficiently stack multiple samples into a batch. 
                To resolve this, we assign a custom padding token. Since GPT-2 already uses "<code>&lt;|endoftext|&gt;</code>" 
                (<code>eos_token</code>) to mark the end of a sequence, 
                we can set it as the padding token to avoid expanding the model's vocabulary.</p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python">from transformers import GPT2Tokenizer

# Define the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("openai-community/gpt2")
# Set EOS token as PAD token to avoid vocabulary resizing
new_pad_token = "&lt;|pad|&gt;"
tokenizer.add_special_tokens({'pad_token': new_pad_token})
# Ensure the tokenizer uses EOS token as PAD token
tokenizer.pad_token = tokenizer.eos_token

# Tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples["text"],                # Tokenize the 'text' column
        padding="max_length",            # Pad all sequences to the same length
        truncation=True,                 # Truncate texts longer than max length
        max_length=1024,                 # Set maximum input length
        return_tensors="pt"              # Return PyTorch tensors
    )
</code></pre>
            </div>
            <p>In <em>causal language modeling</em> (like GPT-2), <code>input_ids</code> are used as labels because 
                the model predicts the next token from previous tokens. So, we need to tokenize the dataset accordingly.
            </p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python"># Function to copy input IDs as labels (for causal language modeling)
def add_labels(example):
    example["labels"] = example["input_ids"].copy()  # Labels match input tokens
    return example
</code></pre>
            </div>
            <p>Now, let's apply our functions to the dataset:</p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python"># Tokenize the dataset
tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,                 # Tokenizes multiple examples at once (faster)
    remove_columns=["text"],      # Removes original text column (not needed)
)

# Add labels to the tokenized dataset
tokenized_datasets = tokenized_datasets.map(
    add_labels,
    batched=True                  # Adds labels for multiple examples
)
</code></pre>
            </div>
        <h2 class="fstyle-secondary">ğŸš€ Step 5: Training the GPT-2 Model with ğŸ¤— <code>Trainer</code></h2>
        <p>
            To train our GPT-2 model, we will use the <strong>Hugging Face <code>Trainer</code></strong> API, which is:
          </p>
          
          <ul>
            <li>âœ… <strong>Simple and Convenient:</strong> Abstracts away boilerplate code.</li>
            <li>âš™ï¸ <strong>Optimized for Transformer Models:</strong> Built specifically for models like GPT-2.</li>
            <li>ğŸš€ <strong>Efficient:</strong> Supports distributed training, mixed-precision, and logging out-of-the-box.</li>
          </ul>
          
          <p>
            However, you can always choose to <strong>train using plain PyTorch</strong> if you prefer full control over the training loop.
          </p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python">from transformers import Trainer

# Initialize the Trainer
trainer = Trainer(
    model=model,                      # Our GPT-2 model
    args=training_args,               # Training arguments (defined previously)
    train_dataset=tokenized_datasets, # Tokenized dataset for training
    tokenizer=tokenizer               # Tokenizer for data preprocessing
)
</code></pre>
            </div>
            <p>With everything set up, we can now start training the model ğŸ’ª:</p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python"># Start the training process
trainer.train()
</code></pre>
            </div>
            <p>Once training is complete, let's save the trained model and tokenizer for future use:</p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python"># Save the trained model
trainer.save_model("./gpt2_pretrained")
# Save the tokenizer with its special tokens
tokenizer.save_pretrained("./gpt2_pretrained")
</code></pre>
            </div>
            <h2 class="fstyle-secondary">ğŸš€ Step 6: Testing the Trained GPT-2 Model & Generating Text</h2>
            <p>Now that we've finished training the GPT-2 model, it's time to test it by generating text! 
                We will use ğŸ¤— <code>pipeline</code>API, which provides an easy interface for model inference.</p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python">from transformers import pipeline

# Load fine-tuned model and tokenizer from saved directory
generator = pipeline(
    "text-generation",                # Task: Generate text
    model="./gpt2_pretrained",        # Path to fine-tuned model
    tokenizer="./gpt2_pretrained"     # Path to saved tokenizer
)
</code></pre>
            </div>
            <p>ğŸ“¦ Here, <code>pipeline</code> is a high-level API from Hugging Face that simplifies model inference. 
                Finally, generating some text:
            </p>
            <div class="code-block language-python">
                <button type="button" class="btn btn-sm btn-outline-secondary copy-btn"
                        onclick="copyCodeToClipboard(this)" aria-label="Copy code">
                    <span class="d-none d-dark">â˜€ï¸</span>
                    <span class="d-none d-light">ğŸŒ™</span>
                    Copy
                </button>
<pre class="bg-body-tertiary rounded"><code class="language-python"># Generate text with the fine-tuned model
output = generator(
    "The equations of motion are",  # Prompt to start generating text
    max_length=50,                 # Maximum length of the generated sequence
    do_sample=True,                # Enable sampling (introduces randomness)
    temperature=1.0                # Sampling temperature (higher = more creative)
)
</code></pre>
            </div>
            <strong>ğŸ“ Explanation of Generation Parameters:</strong>

            <ul>
              <li>ğŸ–‹ï¸ Prompt: <code>"The equations of motion are"</code> â€“ The starting point (also known as a seed or prompt).</li>
              <li>ğŸ“ <code>max_length=50</code> â€“ Limits the output to 50 tokens (including the prompt).</li>
              <li>ğŸ² <code>do_sample=True</code> â€“ Enables <strong>sampling</strong>, making the output less <strong>deterministic</strong> (adds randomness).</li>
              <li>ğŸŒ¡ï¸ <code>temperature=1.0</code> â€“ Controls the randomness:</li>
                </ul>
            
            <ul>
              <li><strong>Higher (&gt;1)</strong> = More creative and diverse output.</li>
              <li><strong>Lower (&lt;1)</strong> = More focused and conservative output.</li>
                </ul>
            
            <h2 class="fstyle-secondary">ğŸ‰ Conclusion: Building and Testing a Custom GPT-2 Model</h2>

            <p>
              Congratulations on training your own GPT-2 model! You've come a long way â€” 
              from preparing your dataset and fine-tuning your model to generating meaningful text. ğŸ’ªğŸ“š
            </p>
                        




            </article>

            <nav class="blog-pagination" aria-label="Pagination"> <a class="btn btn-outline-primary rounded-pill" href="#">Older</a> <a class="btn btn-outline-secondary rounded-pill disabled" aria-disabled="true">Newer</a> </nav>
        </div>
        <div class="col-md-4">
            <div class="position-sticky" style="top: 2rem;">
                <div class="p-4 mb-3 bg-body-tertiary rounded">
                <h4 class="fst-italic">About</h4>
                <p class="mb-0">ğŸ‘‹ Welcome to my blog!<br>
                    This blog is a space where I share reflections, research notes, and ideas at the intersection of particle physics and artificial intelligence.
                </p>
                </div>
                <!--
                <div>
                <h4 class="fst-italic">Recent posts</h4>
                <ul class="list-unstyled">
                    <li>
                        <a class="d-flex flex-column flex-lg-row gap-3 align-items-start align-items-lg-center py-3 link-body-emphasis text-decoration-none border-top" href="#">
                            <svg aria-hidden="true" class="bd-placeholder-img " height="96" preserveAspectRatio="xMidYMid slice" width="100%" xmlns="http://www.w3.org/2000/svg">
                            <rect width="100%" height="100%" fill="#777"></rect>
                            </svg>
                            <div class="col-lg-8">
                            <h6 class="mb-0">Example blog post title</h6>
                            <small class="text-body-secondary">January 15, 2024</small> 
                            </div>
                        </a>
                    </li>
                    <li>
                        <a class="d-flex flex-column flex-lg-row gap-3 align-items-start align-items-lg-center py-3 link-body-emphasis text-decoration-none border-top" href="#">
                            <svg aria-hidden="true" class="bd-placeholder-img " height="96" preserveAspectRatio="xMidYMid slice" width="100%" xmlns="http://www.w3.org/2000/svg">
                            <rect width="100%" height="100%" fill="#777"></rect>
                            </svg>
                            <div class="col-lg-8">
                            <h6 class="mb-0">This is another blog post title</h6>
                            <small class="text-body-secondary">January 14, 2024</small> 
                            </div>
                        </a>
                    </li>
                    <li>
                        <a class="d-flex flex-column flex-lg-row gap-3 align-items-start align-items-lg-center py-3 link-body-emphasis text-decoration-none border-top" href="#">
                            <svg aria-hidden="true" class="bd-placeholder-img " height="96" preserveAspectRatio="xMidYMid slice" width="100%" xmlns="http://www.w3.org/2000/svg">
                            <rect width="100%" height="100%" fill="#777"></rect>
                            </svg>
                            <div class="col-lg-8">
                            <h6 class="mb-0">Longer blog post title: This one has multiple lines!</h6>
                            <small class="text-body-secondary">January 13, 2024</small> 
                            </div>
                        </a>
                    </li>
                </ul>
                </div>
                -->
                <!--
                <div class="p-4">
                <h4 class="fst-italic">Archives</h4>
                <ol class="list-unstyled mb-0">
                    <li><a href="#">March 2021</a></li>
                    <li><a href="#">February 2021</a></li>
                    <li><a href="#">January 2021</a></li>
                    <li><a href="#">December 2020</a></li>
                    <li><a href="#">November 2020</a></li>
                    <li><a href="#">October 2020</a></li>
                    <li><a href="#">September 2020</a></li>
                    <li><a href="#">August 2020</a></li>
                    <li><a href="#">July 2020</a></li>
                    <li><a href="#">June 2020</a></li>
                    <li><a href="#">May 2020</a></li>
                    <li><a href="#">April 2020</a></li>
                </ol>
                </div>
                -->
            </div>
        </div>
    </div>
    </main>
    <footer class="py-2 text-center text-body-secondary bg-body-tertiary">
    <p class="mb-1 small">Â© 2025 Waleed Esmail</p>
    <p class="mb-0 small"> <a href="#">Back to top</a> </p>
    </footer>
    <script src="/docs/5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-FKyoEForCGlyvwx9Hj09JcYn3nv7wiPVlz7YYwJrWVcXK/BmnVDxM+D2scQbITxI" class="astro-vvvwv3sm"></script>
    
    <!-- Universal Code Block Functionality -->
    <script>
    // Universal copy code function
    function copyCodeToClipboard(btn) {
        const pre = btn.parentElement.querySelector('code');
        const code = pre.innerText;
        navigator.clipboard.writeText(code).then(() => {
            const originalText = btn.innerHTML;
            btn.innerHTML = 'Copied!';
            btn.classList.remove('btn-outline-secondary');
            btn.classList.add('btn-success');
            setTimeout(() => {
                btn.innerHTML = originalText;
                btn.classList.add('btn-outline-secondary');
                btn.classList.remove('btn-success');
            }, 1200);
        });
    }
    
    // Theme detection for copy button icons
    function updateCopyBtnTheme() {
        const html = document.documentElement;
        const isDark = html.dataset.bsTheme === 'dark';
        document.querySelectorAll('.code-block button').forEach(btn => {
            const darkIcon = btn.querySelector('.d-dark');
            const lightIcon = btn.querySelector('.d-light');
            if (darkIcon && lightIcon) {
                darkIcon.style.display = isDark ? 'inline' : 'none';
                lightIcon.style.display = !isDark ? 'inline' : 'none';
            }
        });
    }
    
    // Listen to theme changes
    if (window.MutationObserver) {
        const html = document.documentElement;
        const observer = new MutationObserver(updateCopyBtnTheme);
        observer.observe(html, { attributes: true, attributeFilter: ['data-bs-theme'] });
        document.addEventListener('DOMContentLoaded', updateCopyBtnTheme);
    }
    </script>


    <!-- Footer -->
    <div class="container"> 
        <footer class="d-flex flex-wrap justify-content-between align-items-center py-3 my-4 border-top"> 
            <div class="col-4 d-flex align-items-center"> 
                <a href="/" class="mb-3 me-2 mb-md-0 text-body-secondary text-decoration-none lh-1" aria-label="Bootstrap"> 
                    <svg class="bi" width="30" height="24" aria-hidden="true">
                        <use xlink:href="#bootstrap"></use>
                    </svg> 
                </a> 
                <span class="mb-3 mb-md-0 text-body-secondary">Â© 2025 Esmail, Waleed</span> 
            </div> 
        </footer> 
    </div>
    
    <script>
    const html = document.documentElement;
    const btn = document.getElementById('themeToggle');

    // initialize from localStorage or system preference
    const stored = localStorage.getItem('theme');
    const preferDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
    html.dataset.bsTheme = stored || (preferDark ? 'dark' : 'light');
    updateBtn();

    btn.addEventListener('click', () => {
        html.dataset.bsTheme = html.dataset.bsTheme === 'dark' ? 'light' : 'dark';
        localStorage.setItem('theme', html.dataset.bsTheme);
        updateBtn();
    });

    function updateBtn() {
        btn.textContent = html.dataset.bsTheme === 'dark' ? 'â˜€ï¸' : 'ğŸŒ™';
        btn.classList.toggle('btn-outline-light', html.dataset.bsTheme === 'dark');
        btn.classList.toggle('btn-outline-secondary', html.dataset.bsTheme !== 'dark');
    }
    </script>
     <!-- for syntax highlighting -->
     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
     <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
     <script>
     // Force theme-aware code block styling
     function updateCodeBlockTheme() {
         const html = document.documentElement;
         const isDark = html.dataset.bsTheme === 'dark';
         const codeBlocks = document.querySelectorAll('.code-block pre');
         
         codeBlocks.forEach(block => {
             if (isDark) {
                 block.style.backgroundColor = '#1a1a1a';
                 block.style.color = '#e9ecef';
                 block.querySelector('code').style.backgroundColor = '#1a1a1a';
                 block.querySelector('code').style.color = '#e9ecef';
             } else {
                 block.style.backgroundColor = '#f8f9fa';
                 block.style.color = '#212529';
                 block.querySelector('code').style.backgroundColor = '#f8f9fa';
                 block.querySelector('code').style.color = '#212529';
             }
         });
     }
     
     // Run on page load
     document.addEventListener('DOMContentLoaded', function() {
         hljs.highlightAll();
         updateCodeBlockTheme();
     });
     
     // Listen for theme changes
     if (window.MutationObserver) {
         const html = document.documentElement;
         const observer = new MutationObserver(updateCodeBlockTheme);
         observer.observe(html, { attributes: true, attributeFilter: ['data-bs-theme'] });
     }
     </script>
</body>    

</html>